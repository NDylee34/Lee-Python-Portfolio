# Lee-Python-Portfolio

This portfolio is a compilation of all the **Elements of Computing II projects** I have worked on this semester. It is regularly updated to showcase **data analysis, visualization, and interactive applications** using Python.

## 📌 Projects

### **Streamlit App**
🔗 [View Project Repository](https://github.com/NDylee34/Lee-Python-Portfolio/tree/main/basic-streamlit-app)

**Description:**  
The **Streamlit App** is an **interactive web application** that allows users to **explore data about penguins inhabiting different islands**. The app:

✔ Loads **penguin dataset** from a CSV file.  
✔ Displays **a searchable and filterable table**.  
✔ Allows users to **filter data by island using a dropdown menu**.  
✔ Uses **Streamlit** to create a **dynamic and user-friendly web interface**.

**Refinements to Improve the App:**  
✅ Enhance user experience by **adding data visualizations** (e.g., distribution of species).  
✅ Implement **dynamic filtering** for multiple columns.  
✅ Improve the UI with **Streamlit styling options**.

### **Tidy Data Project**
🔗 [View Project Repository](https://github.com/NDylee34/Lee-Python-Portfolio/tree/main/TidyData-Project)

**Description:**  
The **Tidy Data Project** applies **tidy data principles** to transform and analyze a dataset of **2008 Olympics medalists**. This project involves:

✔ **Data Cleaning & Reshaping** – Converting the dataset into a **structured format**.  
✔ **Exploratory Data Analysis** – Identifying trends in **gender distribution, medal counts, and country rankings**.  
✔ **Advanced Visualizations** – Creating **bar charts, boxplots, and pivot tables** to **deepen insights**.  

**How It Complements My Portfolio:**  
This project **showcases my ability to efficiently process and analyze unstructured data**, transforming it into a structured format that facilitates deeper insights. By applying tidy data principles, I ensure that the data is **clean, well-organized, and optimized** for analysis. Additionally, I **leverage data visualization techniques** to create compelling and meaningful representations that effectively highlight **trends, patterns, and relationships** within large datasets. Through this approach, I demonstrate my proficiency in making data-driven insights accessible and impactful.

You're right! Here's the same emoji-enhanced section delivered in plain text for **easy copy-paste** into your `.Rmd` or `README.md` file — no formatting issues:

### **Named Entity Recognition (NER) Streamlit App**  
🔗 [View Project Repository](https://github.com/NDylee34/Lee-Python-Portfolio/tree/main/NERStreamlitApp)  
🚀 [Launch Live App](https://lee-ner.streamlit.app/)

**Description:**  
The **NER Streamlit App** is a fully interactive web application that enables users to perform **custom rule-based Named Entity Recognition (NER)** using **spaCy**. Designed for flexibility and ease of use, the app allows users to:

✔ **Upload or Input Custom Text** – Analyze any document by uploading `.txt` files or entering text directly.  
✔ **Define Custom Entity Labels & Patterns** – Use spaCy’s `EntityRuler` to create custom rules for identifying names, organizations, or key terms.  
✔ **Visualize Results Instantly** – View entities in both a labeled table and a live text highlight using spaCy’s `displacy`.  
✔ **Fully Cloud-Hosted** – Deployed via **Streamlit Community Cloud** for public access and sharing.

**How It Complements My Portfolio:**  
This project showcases my ability to combine **natural language processing**, **interactive web development**, and **cloud deployment** into one cohesive, user-friendly product. It highlights my strengths in:

- 🧩 **Rule-based NLP** and custom pattern design using spaCy  
- 🧑‍💻 **Streamlit development** for interactive interfaces  
- 📨 **User input handling** and real-time content generation  
- ☁️ **Deployment to Streamlit Cloud** for accessibility and product-readiness

In the context of my broader portfolio, this app reflects my skill in building tools that are **functionally reliable** and **accessible to non-technical users**. It adds a new layer to my work in data analysis by incorporating **text processing, pattern recognition, and frontend user experience**, demonstrating my technical ability across the data pipeline.